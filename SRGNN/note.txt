### main_srgnn.py ###
import argparse
import random
from tqdm import tqdm
from pathlib import Path
import os
import pickle
import numpy as np
import time

import torch 
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, SequentialSampler

from srgnn import SRGNN
from collate import (collate_fn_factory, seq_to_session_graph)
import metric
from dataset import load_data,RecSysDataset
import warnings 
warnings.filterwarnings('ignore')

def seed_everything(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

def prepare_batch(batch, device):
    inputs, labels = batch
    # inputs, labels = batch
    inputs_gpu  = [x.to(device) for x in inputs]
    labels_gpu  = labels.to(device)
   
    return inputs_gpu, labels_gpu 

def validate(valid_loader, model,device):
    model.eval()
    recalls = []
    mrrs = []
    losses=[]
    with torch.no_grad():
        for step, batch in tqdm(enumerate(valid_loader), total=len(valid_loader),position=0,leave=True):
            inputs, labels = prepare_batch(batch, device)
            outputs = model(*inputs)
            criterion = nn.CrossEntropyLoss()
            loss = criterion(outputs, labels)
            # loss = nn.functional.nll_loss(outputs, labels)
            logits = F.softmax(outputs, dim = 1)
            recall, mrr = metric.evaluate(logits, labels, k = args.topk)
            recalls.append(recall)
            mrrs.append(mrr)
            losses.append(loss.item())
    
    mean_recall = np.mean(recalls)
    mean_mrr = np.mean(mrrs)
    mean_loss=np.mean(losses)
    
    return mean_recall, mean_mrr, mean_loss

def fix_weight_decay(model):
    decay = []
    no_decay = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        if any(map(lambda x: x in name, ['bias', 'batch_norm', 'activation'])):
            no_decay.append(param)
        else:
            decay.append(param)
    params = [{'params': decay}, {'params': no_decay, 'weight_decay': 0}]
    return params

def split_validation(train_data, valid_portion):
    train_x, train_y = train_data
    n_samples = len(train_x)
    sidx = np.arange(n_samples, dtype='int32')
    np.random.seed(101)
    np.random.shuffle(sidx)
    n_train = int(np.round(n_samples * (1. - valid_portion)))
    valid_x = [train_x[s] for s in sidx[n_train:]]
    valid_y = [train_y[s] for s in sidx[n_train:]]
    train_x = [train_x[s] for s in sidx[:n_train]]
    train_y = [train_y[s] for s in sidx[:n_train]]  
    
    train_set=(train_x, train_y)
    valid_set=(valid_x, valid_y)
    
    return train_set, valid_set

def main(args,device):
    
    # if not os.path.exists(os.path.join(os.getcwd(),"long_seq")):
    #     os.makedirs("long_seq")
    # if not os.path.exists(os.path.join(os.getcwd(),"short_seq")):
    #     os.makedirs("short_seq")
        
    train, test = load_data(args.dataset_dir)
    if args.validation:
        train,valid=split_validation(train, args.valid_split)
        test=valid
    
    train_data = RecSysDataset(train)
    test_data = RecSysDataset(test)

    collate_fn = collate_fn_factory(seq_to_session_graph)

    train_loader = DataLoader(
        train_data,
        batch_size=args.batch_size,
        # shuffle=True,  # Remove shuffle=True in this case as SubsetRandomSampler shuffles data already
        # drop_last=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        sampler=SequentialSampler(train_data)
    )


    test_loader = DataLoader(
        test_data,
        batch_size=args.batch_size,
        # shuffle=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn
    )
    print()
    print('{:<30}{:<10,} '.format("training mini-batch",len(train_loader)))
    print('{:<30}{:<10,} '.format("test mini-batch",len(test_loader)))
    
    model = SRGNN(args.n_items, args.embedding_dim, args.num_layers, feat_drop=args.dropout_feat)
    model = model.to(device)
    print()
    print("{:<30}{:<20,}".format("Number of parameters",np.sum([p.nelement() for p in model.parameters()])))
    
    if args.weight_decay > 0:
        params = fix_weight_decay(model)
    else:
        params = model.parameters()

    optimizer = optim.Adam(params, args.lr, weight_decay=args.weight_decay)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_dc_step, gamma=args.lr_dc)
    
    
    best_result = [0, 0]
    best_epoch = [0, 0]
    bad_counter = 0
    
    # best_metric = float('inf') ## if cross-entropy loss is selected
    best_mrr = float(0) 
    best_recall = float(0)

    TRAIN_LOSS=[]
    TEST_LOSS=[]

    TRAIN_RECALL=[]
    TEST_RECALL=[]
    
    TRAIN_MRR=[]
    TEST_MRR=[]
    
    for epoch in tqdm(range(args.epochs)): #before: no leave param, now , leave=False

        scheduler.step(epoch = epoch)
        # trainForEpoch(train_loader, model, optimizer, epoch, args.epochs, criterion, device, log_aggr = 1)
        model.train()
        sum_epoch_loss = 0
        start = time.time()
        for step, batch in tqdm(enumerate(train_loader), total=len(train_loader),position=0,leave=True):
            inputs, labels = prepare_batch(batch, device)

            optimizer.zero_grad()
            logits = model(*inputs)
            loss = criterion(logits, labels)
            # loss = nn.functional.nll_loss(logits, labels)

            loss.backward()
            if args.gradient_accumulation:
                if (step+1)%args.accumulation_steps == 0 or step==len(train_loader):
                    optimizer.step()
            else:
                optimizer.step()
 
            loss_val = loss.item()
            sum_epoch_loss += loss_val

#             if (step+1)%(len(train_loader)//args.log_aggr) == 0:
#                 print('Epoch {:05d} |  Loss {:.4f} | Speed (samples/sec) {:.2f}'
#                       .format(epoch, sum_epoch_loss / (step + 1), labels.shape[0] / (time.time() - start)))

#             start = time.time()
        

        root_dir=os.path.join(os.getcwd(),"output_metrics")
        if not os.path.exists(root_dir):
            os.makedirs(root_dir)

        train_recall, train_mrr, train_loss = validate(train_loader, model, device)
        TRAIN_LOSS.append(train_loss)
        TRAIN_MRR.append(train_mrr)
        TRAIN_RECALL.append(train_recall)
        print()
        print('Epoch {} training--loss: {:.4f}, Recall@{}: {:.4f}, MRR@{}: {:.4f} \n'\
              .format(epoch, train_loss,args.topk, train_recall, args.topk, train_mrr))

        # with open(os.path.join(os.getcwd(),"output_metrics","train_"+args.output_name),'a') as f:
        #     f.write(f'{epoch+1},{train_recall},{train_mrr}\n')
                  
        test_recall, test_mrr, test_loss = validate(test_loader, model, device)
        TEST_LOSS.append(test_loss)
        TEST_MRR.append(test_mrr)
        TEST_RECALL.append(test_recall)
        print('Epoch {} test--loss: {:.4f}, Recall@{}: {:.4f}, MRR@{}: {:.4f} \n'\
              .format(epoch, test_loss,args.topk, test_recall, args.topk, test_mrr))
        
        root_dir=os.path.join(os.getcwd(),"output_metrics")
        with open(os.path.join(root_dir,"test_"+args.output_name),'a') as f:
            f.write(f'{epoch+1},{test_recall},{test_mrr}\n')

        # store best loss and save a model checkpoint
        ckpt_dict = {
            'epoch': epoch + 1,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict()
        }
        
        if test_recall>best_recall or test_mrr>best_mrr:
            best_recall=test_recall
            best_mrr=test_mrr

            torch.save(ckpt_dict, args.model_checkpoint)          
                
        flag = 0
        if round(test_recall,2) > round(best_result[0],2):
            best_result[0] = test_recall
            best_epoch[0] = epoch
            flag = 1
        if round(test_mrr,2) > round(best_result[1],2):
            best_result[1] = test_mrr
            best_epoch[1] = epoch
            flag = 1
        print('Best Result:')
        print('\tRecall@20:\t%.4f\tMMR@20:\t%.4f\tEpoch:\t%d,\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))
        bad_counter += 1 - flag
        if bad_counter >= args.patience:
            break
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument(
        '--dataset-dir', default='../data/', help='the dataset directory'
    )
    parser.add_argument("--seed",  type=int,default=101,
            help="random seed for np.random.seed, torch.manual_seed and torch.cuda.manual_seed.")
    parser.add_argument('--n_items', type=int, default=25060)
    parser.add_argument('--embedding-dim', type=int, default=128, help='the embedding size')
    parser.add_argument('--num-layers', type=int, default=1, help='the number of layers')
    parser.add_argument('--dropout_feat', type=float, default=0.1, help='the dropout ratio for features')
    parser.add_argument('--lr', type=float, default=1e-3, help='the learning rate')
    parser.add_argument('--lr_dc', type=float, default=0.1, help='learning rate decay rate')
    parser.add_argument('--lr_dc_step', type=int, default=3, help='the number of steps after which the learning rate decay')
    parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')  # [0.001, 0.0005, 0.0001, 0.00005, 0.00001]
    parser.add_argument("--gradient_accumulation",action='store_true', help='gradient accumulation or not')
    parser.add_argument("--accumulation_steps",type=int,default=2,
                               help="Number of updates steps to accumulate before performing a backward/update pass.")
    
    parser.add_argument('--step', type=int, default=1, help='gnn propogation steps')
    parser.add_argument('--patience', type=int, default=5, help='the number of epoch to wait before early stop ')
    parser.add_argument('--batch-size', type=int, default=100, help='the batch size for training')
    parser.add_argument('--epochs', type=int, default=30, help='the number of training epochs')
    parser.add_argument("--output_name", type=str, default="weblog_metrics.txt")
    parser.add_argument("--model_checkpoint", type=str, default="weblog_checkpoint.pth")    
    parser.add_argument('--weight-decay',type=float,default=1e-5,help='the parameter for L2 regularization')
    parser.add_argument('--validation', action='store_true', help='validation')
    parser.add_argument('--valid-split',type=float,default=0.1,help='the fraction for the validation set')
    parser.add_argument('--topk', type=int, default=20, help='number of top score items selected for calculating recall and mrr metrics')
    parser.add_argument('--log_aggr', type=int, default=1, help='print the loss after this number of iterations')
    parser.add_argument('--num-workers',type=int,default=0,help='the number of processes to load the input graphs')

    args= parser.parse_args()
    print(args)
    
    seed_everything(args.seed)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    main(args,device)

### srgnn.py ###
import math

import torch as th
import torch.nn as nn
import torch.nn.functional as F

import dgl
import dgl.ops as F
import dgl.function as fn

class SRGNNLayer(nn.Module):
    def __init__(self, input_dim, output_dim, batch_norm=False, feat_drop=0.0, activation=None):
        super().__init__()
        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None
        self.dropout    = nn.Dropout(feat_drop)
        self.gru        = nn.GRUCell(2 * input_dim, output_dim)
        self.W1         = nn.Linear(input_dim, output_dim, bias=False)
        self.W2         = nn.Linear(input_dim, output_dim, bias=False)
        self.activation = activation
        
    def messager(self, edges):

        return {'m': edges.src['ft'] * edges.data['w'].unsqueeze(-1), 'w': edges.data['w']}

    def reducer(self, nodes):
        m = nodes.mailbox['m']
        w = nodes.mailbox['w']
        hn = m.sum(dim=1) / w.sum(dim=1).unsqueeze(-1)
        return {'neigh': hn}
    
    def forward(self, mg, feat):
        with mg.local_scope():
            if self.batch_norm is not None:
                feat = self.batch_norm(feat)
            mg.ndata['ft'] = self.dropout(feat)
            if mg.number_of_edges() > 0:
                mg.update_all(self.messager, self.reducer)
                neigh1 = mg.ndata['neigh']
                mg1 = mg.reverse(copy_edata=True)
                mg1.update_all(self.messager, self.reducer)
                neigh2 = mg1.ndata['neigh']
                neigh1 = self.W1(neigh1)
                neigh2 = self.W2(neigh2)
                hn = th.cat((neigh1, neigh2), dim=1)
                rst = self.gru(hn, feat)
            else:
                #rst = self.gru(th.cat((feat, feat), dim=1), feat)
                rst = feat
        if self.activation is not None:
            rst = self.activation(rst)
        return rst
    
class AttnReadout(nn.Module):
    def __init__(
        self,
        input_dim,
        hidden_dim,
        output_dim,
        batch_norm=True,
        feat_drop=0.0,
        activation=None,
    ):
        super().__init__()
        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None
        self.feat_drop = nn.Dropout(feat_drop)
        self.fc_u = nn.Linear(input_dim, hidden_dim, bias=False)
        self.fc_v = nn.Linear(input_dim, hidden_dim, bias=True)
        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)
        self.fc_out = (
            nn.Linear(input_dim, output_dim, bias=False)
            if output_dim != input_dim
            else None
        )
        self.activation = activation

    def forward(self, g, feat, last_nodes):
        if self.batch_norm is not None:
            feat = self.batch_norm(feat)
        feat = self.feat_drop(feat)
        feat_u = self.fc_u(feat)
        feat_v = self.fc_v(feat[last_nodes])
        feat_v = dgl.broadcast_nodes(g, feat_v)
        e = self.fc_e(th.sigmoid(feat_u + feat_v)) 
        alpha = F.segment.segment_softmax(g.batch_num_nodes(), e) 
        feat_norm = feat * alpha
        rst = F.segment.segment_reduce(g.batch_num_nodes(), feat_norm, 'sum')
        if self.fc_out is not None:
            rst = self.fc_out(rst)
        if self.activation is not None:
            rst = self.activation(rst)
        return rst

class SRGNN(nn.Module):
    
    def __init__(self, num_items, embedding_dim, num_layers, feat_drop=0.0):
        super().__init__()
        self.embedding = nn.Embedding(num_items, embedding_dim)
        # self.indices = th.arange(num_items, dtype=th.long)
        self.register_buffer('indices', th.arange(num_items, dtype=th.long))
        self.embedding_dim = embedding_dim
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        input_dim = embedding_dim
        for i in range(num_layers):
            layer = SRGNNLayer(
                input_dim,
                embedding_dim,
                batch_norm=None,
                feat_drop=feat_drop
            )
            self.layers.append(layer)
        self.readout = AttnReadout(
            input_dim,
            embedding_dim,
            embedding_dim,
            batch_norm=None,
            feat_drop=feat_drop,
            activation=None,
        )
        input_dim += embedding_dim
        self.feat_drop = nn.Dropout(feat_drop)
        self.fc_sr = nn.Linear(input_dim, embedding_dim, bias=False)
        
        self.reset_parameters()
        
    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.embedding_dim)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)
        
    def forward(self, mg, sg=None):
        iid = mg.ndata['iid']

        # if iid.numel() > 0: # Check if the tensor is not empty
        #     iid_min_val = iid.min().item()
        #     iid_max_val = iid.max().item()
        #     num_embeddings_in_layer = self.embedding.num_embeddings
        #     print(f"Embedding layer num_embeddings (expected max index + 1): {num_embeddings_in_layer}")
        #     if iid_max_val >= num_embeddings_in_layer:
        #         print(f"ERROR: Maximum value in iid ({iid_max_val}) is OUT OF BOUNDS for embedding layer (size {num_embeddings_in_layer}). Expected max index is {num_embeddings_in_layer - 1}.")
        #         # Optionally, find and print the specific offending indices:
        #         # offending_indices = iid[iid >= num_embeddings_in_layer]
        #         # print(f"Offending iid values (>= {num_embeddings_in_layer}): {offending_indices}")
        #     if iid_min_val < 0:
        #         print(f"ERROR: Minimum value in iid ({iid_min_val}) is OUT OF BOUNDS (negative).")
        #     else:
        #         print("iid tensor is empty!")

        feat = self.feat_drop(self.embedding(iid))
        
        out = feat
        for i, layer in enumerate(self.layers):
            out = layer(mg, out)

        last_nodes = mg.filter_nodes(lambda nodes: nodes.data['last'] == 1)
        
        sr_g = self.readout(mg, feat, last_nodes)
        sr_l = feat[last_nodes]
        sr = th.cat([sr_l, sr_g], dim=1)
        sr = self.fc_sr(sr)
        target = self.embedding(self.indices)
        logits = sr @ target.t()
        logits = th.log(nn.functional.softmax(logits, dim=-1))
        return logits# , 0



### save embedding table ###

def dump_item_embeddings(model: SRGNN, path: str):
    """
    Save the item-embedding matrix as a CPU tensor *and* a .npy file.
    """
    emb = model.embedding.weight.detach().cpu()        # (n_items, d)
    torch.save(emb, f"{path}.pt")
    np.save(f"{path}.npy", emb.numpy())

if test_recall > best_recall or test_mrr > best_mrr:
    best_recall = test_recall
    best_mrr    = test_mrr
    torch.save(ckpt_dict, args.model_checkpoint)

    # NEW: persist embedding table
    dump_item_embeddings(model, Path(args.model_checkpoint).with_suffix(''))

item_emb = torch.load("weblog_checkpoint.pt")   # shape: (n_items, d)
# or
item_emb = torch.from_numpy(np.load("weblog_checkpoint.npy"))

