{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6886a2ad-bcc2-4f1b-a14e-d17b555417de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/SRGNN/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "from srgnn import SRGNN\n",
    "from collate import (collate_fn_factory, seq_to_session_graph)\n",
    "import metric\n",
    "from dataset import load_data,RecSysDataset\n",
    "# from train import TrainRunner\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0152b81c-e850-4564-a3b3-f3d533cc0377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, dataset_dir='../dataset/amex_explorepoi-poi_category/', embedding_dim=256, epochs=3, feat_drop=0.1, log_aggr=1, lr=0.001, n_items=556, num_layers=1, num_workers=0, topk=20, valid_split=0.1, weight_decay=0.0001)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\n",
    "    '--dataset-dir', default='../YOOCHOOSE_data/yoochoose1_64/', help='the dataset directory'\n",
    ")\n",
    "parser.add_argument('--n_items', type=int, default=37484, help='number of unique items. 37484 for yoochoose')\n",
    "parser.add_argument('--embedding-dim', type=int, default=256, help='the embedding size')\n",
    "parser.add_argument('--num-layers', type=int, default=1, help='the number of layers')\n",
    "parser.add_argument('--feat-drop', type=float, default=0.1, help='the dropout ratio for features')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='the learning rate')\n",
    "parser.add_argument(\n",
    "    '--batch-size', type=int, default=512, help='the batch size for training'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--epochs', type=int, default=30, help='the number of training epochs'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--weight-decay',\n",
    "    type=float,\n",
    "    default=1e-4,\n",
    "    help='the parameter for L2 regularization',\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--valid-split',\n",
    "    type=float,\n",
    "    default=0.1,\n",
    "    help='the fraction for the validation set',\n",
    ")\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "    '--topk', \n",
    "    type=int, \n",
    "    default=20, \n",
    "    help='number of top score items selected for calculating recall and mrr metrics',\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--log_aggr', \n",
    "    type=int, \n",
    "    default=1, \n",
    "    help='print the loss after this number of iterations',\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--num-workers',\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help='the number of processes to load the input graphs',\n",
    ")\n",
    "    \n",
    "args,_ = parser.parse_known_args()\n",
    "\n",
    "args.dataset_dir=\"../dataset/amex_explorepoi-poi_category/\"\n",
    "args.batch_size=64\n",
    "args.n_items=556\n",
    "args.epochs=3\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fd013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23bf502-b021-49d9-802b-7ba54aa4d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Dataset info:\n",
      "Number of sessions: 3182\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Dataset info:\n",
      "Number of sessions: 354\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Dataset info:\n",
      "Number of sessions: 305\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = load_data(args.dataset_dir, valid_portion=args.valid_split)\n",
    "\n",
    "train_data = RecSysDataset(train)\n",
    "valid_data = RecSysDataset(valid)\n",
    "test_data = RecSysDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449d7b0-6217-4eb0-8379-5bd49996767b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ccd7e8c-76ef-4eed-a947-9f3ac8a6b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mini-batch           50         \n",
      "validation mini-batch         6          \n",
      "test mini-batch               5          \n"
     ]
    }
   ],
   "source": [
    "collate_fn = collate_fn_factory(seq_to_session_graph)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=args.batch_size,\n",
    "    # shuffle=True,\n",
    "    # drop_last=True,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    sampler=SequentialSampler(train_data)\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_data,\n",
    "    batch_size=args.batch_size,\n",
    "    # shuffle=True,\n",
    "    # drop_last=True,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    sampler=SequentialSampler(valid_data)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=args.batch_size,\n",
    "    # shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print('{:<30}{:<10,} '.format(\"training mini-batch\",len(train_loader)))\n",
    "print('{:<30}{:<10,} '.format(\"validation mini-batch\",len(valid_loader)))\n",
    "print('{:<30}{:<10,} '.format(\"test mini-batch\",len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a6e588-42f0-4a04-8a33-8213790a1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters          1,127,424           \n"
     ]
    }
   ],
   "source": [
    "model = SRGNN(args.n_items, args.embedding_dim, args.num_layers, feat_drop=args.feat_drop)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device=torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"{:<30}{:<20,}\".format(\"Number of parameters\",np.sum([p.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297eebe4-e49d-4e05-a908-84c4280795cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch, device):\n",
    "    inputs, labels = batch\n",
    "    # inputs, labels = batch\n",
    "    inputs_gpu  = [x.to(device) for x in inputs]\n",
    "    labels_gpu  = labels.to(device)\n",
    "   \n",
    "    return inputs_gpu, labels_gpu \n",
    "\n",
    "def trainForEpoch(train_loader, model, optimizer, epoch, num_epochs, criterion,device,log_aggr=10):\n",
    "    model.train()\n",
    "\n",
    "    sum_epoch_loss = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for i, batch in tqdm(enumerate(train_loader), total=len(train_loader),position=0,leave=True):\n",
    "        inputs, labels = prepare_batch(batch, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(*inputs)\n",
    "        # loss = criterion(logits, labels)\n",
    "        loss = nn.functional.nll_loss(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        loss_val = loss.item()\n",
    "        sum_epoch_loss += loss_val\n",
    "\n",
    "        if i%(len(train_loader)//log_aggr) == 0 and not i==0:\n",
    "            print('[TRAIN] epoch %d/%d batch loss: %.4f (avg %.4f) (%.2f im/s)'\n",
    "                % (epoch + 1, num_epochs, loss_val, sum_epoch_loss / (i + 1),\n",
    "                  len(inputs) / (time.time() - start)))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "\n",
    "def validate(valid_loader, model,device):\n",
    "    model.eval()\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    losses=[]\n",
    "    with torch.no_grad():\n",
    "        for step, batch in tqdm(enumerate(valid_loader), total=len(valid_loader),position=0,leave=True):\n",
    "            inputs, labels = prepare_batch(batch, device)\n",
    "            outputs = model(*inputs)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            loss = nn.functional.nll_loss(outputs, labels)\n",
    "            logits = F.softmax(outputs, dim = 1)\n",
    "            recall, mrr = metric.evaluate(logits, labels, k = args.topk)\n",
    "            recalls.append(recall)\n",
    "            mrrs.append(mrr)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_mrr = np.mean(mrrs)\n",
    "    mean_loss=np.mean(losses)\n",
    "    \n",
    "    return mean_recall, mean_mrr, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb3f4881-0b3b-4c9c-979e-3d57ec70d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 39.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 |  Loss 1.6379 | Speed (samples/sec) 2069.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 44.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 training--loss: 1.3648, Recall@20: 0.9863, MRR@20: 0.7373 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 45.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 validation--loss: 2.2234, Recall@20: 0.9153, MRR@20: 0.6358 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 40.86it/s]\n",
      " 33%|███▎      | 1/3 [00:02<00:05,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 test--loss: 3.6011, Recall@20: 0.7964, MRR@20: 0.4331 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 37.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002 |  Loss 1.3860 | Speed (samples/sec) 2057.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 43.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 training--loss: 1.2291, Recall@20: 0.9930, MRR@20: 0.7520 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 44.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation--loss: 2.1232, Recall@20: 0.9257, MRR@20: 0.6500 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 40.29it/s]\n",
      " 67%|██████▋   | 2/3 [00:05<00:02,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 test--loss: 3.3978, Recall@20: 0.8205, MRR@20: 0.4603 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 39.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003 |  Loss 1.2922 | Speed (samples/sec) 2062.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 40.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 training--loss: 1.1513, Recall@20: 0.9969, MRR@20: 0.7616 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 45.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation--loss: 2.0932, Recall@20: 0.9358, MRR@20: 0.6654 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 40.58it/s]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 test--loss: 3.2142, Recall@20: 0.8318, MRR@20: 0.4706 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def fix_weight_decay(model):\n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if any(map(lambda x: x in name, ['bias', 'batch_norm', 'activation'])):\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            decay.append(param)\n",
    "    params = [{'params': decay}, {'params': no_decay, 'weight_decay': 0}]\n",
    "    return params\n",
    "\n",
    "if args.weight_decay > 0:\n",
    "    params = fix_weight_decay(model)\n",
    "else:\n",
    "    params = model.parameters()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "best_metric = float('inf') ## if cross-entropy loss is selected\n",
    "\n",
    "TRAIN_LOSS=[]\n",
    "VALID_LOSS=[]\n",
    "TEST_LOSS=[]\n",
    "\n",
    "TRAIN_MRR=[]\n",
    "VALID_MRR=[]\n",
    "TEST_MRR=[]\n",
    "\n",
    "TRAIN_RECALL=[]\n",
    "VALID_RECALL=[]\n",
    "TEST_RECALL=[]\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)): #before: no leave param, now , leave=False\n",
    "\n",
    "    scheduler.step(epoch = epoch)\n",
    "    # trainForEpoch(train_loader, model, optimizer, epoch, args.epochs, criterion, device, log_aggr = 1)\n",
    "    model.train()\n",
    "    sum_epoch_loss = 0\n",
    "    start = time.time()\n",
    "    for step, batch in tqdm(enumerate(train_loader), total=len(train_loader),position=0,leave=True):\n",
    "        inputs, labels = prepare_batch(batch, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(*inputs)\n",
    "        # loss = criterion(logits, labels)\n",
    "        loss = nn.functional.nll_loss(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        loss_val = loss.item()\n",
    "        sum_epoch_loss += loss_val\n",
    "        \n",
    "        if (step+1)%(len(train_loader)//args.log_aggr) == 0:\n",
    "            print('Epoch {:05d} |  Loss {:.4f} | Speed (samples/sec) {:.2f}'\n",
    "                  .format(epoch + 1, sum_epoch_loss / (step + 1), labels.shape[0] / (time.time() - start)))\n",
    "            \n",
    "        start = time.time()\n",
    "        \n",
    "    if not os.path.exists(os.path.join(os.getcwd(),\"output_metrics\")):\n",
    "        os.makedirs(\"output_metrics\")\n",
    "    \n",
    "    train_recall, train_mrr, train_loss = validate(train_loader, model, device)\n",
    "    TRAIN_LOSS.append(train_loss)\n",
    "    TRAIN_MRR.append(train_mrr)\n",
    "    TRAIN_RECALL.append(train_recall)\n",
    "    print()\n",
    "    print('Epoch {} training--loss: {:.4f}, Recall@{}: {:.4f}, MRR@{}: {:.4f} \\n'\\\n",
    "          .format(epoch, train_loss,args.topk, train_recall, args.topk, train_mrr))\n",
    "    \n",
    "    dataset_name = args.dataset_dir.split('/')[-2]\n",
    "    with open(os.path.join(os.getcwd(),\"output_metrics\",dataset_name+\"_train_metrics.txt\"),'a') as f:\n",
    "        f.write(f'{epoch+1},{train_loss},{train_recall},{train_mrr}\\n')\n",
    "\n",
    "    valid_recall, valid_mrr, valid_loss = validate(valid_loader, model, device)\n",
    "    VALID_LOSS.append(valid_loss)\n",
    "    VALID_MRR.append(valid_mrr)\n",
    "    VALID_RECALL.append(valid_recall)\n",
    "    print('Epoch {} validation--loss: {:.4f}, Recall@{}: {:.4f}, MRR@{}: {:.4f} \\n'\\\n",
    "          .format(epoch, valid_loss,args.topk, valid_recall, args.topk, valid_mrr))\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(),\"output_metrics\",dataset_name+\"_valid_metrics.txt\"),'a') as f:\n",
    "        f.write(f'{epoch+1},{valid_loss},{valid_recall},{valid_mrr}\\n')\n",
    "        \n",
    "    test_recall, test_mrr, test_loss = validate(test_loader, model, device)\n",
    "    TEST_LOSS.append(test_loss)\n",
    "    TEST_MRR.append(test_mrr)\n",
    "    TEST_RECALL.append(test_recall)\n",
    "    print('Epoch {} test--loss: {:.4f}, Recall@{}: {:.4f}, MRR@{}: {:.4f} \\n'\\\n",
    "          .format(epoch, test_loss,args.topk, test_recall, args.topk, test_mrr))\n",
    "\n",
    "    with open(os.path.join(os.getcwd(),\"output_metrics\",dataset_name+\"_test_metrics.txt\"),'a') as f:\n",
    "        f.write(f'{epoch+1},{test_loss},{test_recall},{test_mrr}\\n')\n",
    "        \n",
    "    # writer.add_scalar(\"Recall/train\", test_recall, epoch)\n",
    "\n",
    "    # store best loss and save a model checkpoint\n",
    "    ckpt_dict = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    selected_metric=valid_loss\n",
    "    if selected_metric<best_metric:\n",
    "        best_metric=selected_metric\n",
    "        dataset_name = args.dataset_dir.split('/')[-2] \n",
    "        torch.save(ckpt_dict, dataset_name + '_' + 'latest_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5690d1a1-c377-4e3f-a156-f917bced844c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/sequence-based-recommendation/SRGNN/output_metrics/amex_explorepoi-poi_category_train_metrics.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(),\"output_metrics\",dataset_name+\"_train_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78992800-ba9f-469d-a454-713c480cd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(os.getcwd(),\"output_metrics\")):\n",
    "    os.makedirs(\"output_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ea344-dbff-489d-a800-a473feb4dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1%651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af472db-6b9c-4827-9f89-cd4537f8763c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec923c-845f-4e13-8ff8-2bc53a9b3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = TrainRunner(\n",
    "    args.dataset_dir,\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    device=device,\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    "    patience=args.patience,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c13b6-338f-4e21-ac83-6b6766ecb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start training')\n",
    "mrr, hit = runner.train(args.epochs, args.log_interval)\n",
    "print('MRR@20\\tHR@20')\n",
    "print(f'{mrr * 100:.3f}%\\t{hit * 100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0224bb5-a8fc-41e2-b938-81f189c6b1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba5e7588-77fd-40f2-baac-6c8b998eb501",
   "metadata": {},
   "source": [
    "### model desection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b5836-86ee-47a2-912e-946301635b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.ops as F\n",
    "import dgl.function as fn\n",
    "\n",
    "class SRGNNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, batch_norm=False, feat_drop=0.0, activation=None):\n",
    "        super().__init__()\n",
    "        self.dropout    = nn.Dropout(feat_drop)\n",
    "        self.gru        = nn.GRUCell(2 * input_dim, output_dim)\n",
    "        self.W1         = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.W2         = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def messager(self, edges):\n",
    "        \n",
    "        return {'m': edges.src['ft'] * edges.data['w'].unsqueeze(-1), 'w': edges.data['w']}\n",
    "\n",
    "    def reducer(self, nodes):\n",
    "        \n",
    "        m = nodes.mailbox['m']\n",
    "        w = nodes.mailbox['w']\n",
    "        hn = m.sum(dim=1) / w.sum(dim=1).unsqueeze(-1)\n",
    "        \n",
    "        return {'neigh': hn}\n",
    "    \n",
    "    def forward(self, mg, feat):\n",
    "        with mg.local_scope():\n",
    "            mg.ndata['ft'] = self.dropout(feat)\n",
    "            if mg.number_of_edges() > 0:\n",
    "                mg.update_all(self.messager, self.reducer)\n",
    "                neigh1 = mg.ndata['neigh']\n",
    "                mg1 = mg.reverse(copy_edata=True)\n",
    "                mg1.update_all(self.messager, self.reducer)\n",
    "                neigh2 = mg1.ndata['neigh']\n",
    "                neigh1 = self.W1(neigh1)\n",
    "                neigh2 = self.W2(neigh2)\n",
    "                hn = torch.cat((neigh1, neigh2), dim=1)\n",
    "                rst = self.gru(hn, feat) \n",
    "            else:\n",
    "                rst = feat\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "        return rst\n",
    "    \n",
    "class AttnReadout(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        batch_norm=True,\n",
    "        feat_drop=0.0,\n",
    "        activation=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.fc_u = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.fc_v = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.fc_out = (\n",
    "            nn.Linear(input_dim, output_dim, bias=False)\n",
    "            if output_dim != input_dim\n",
    "            else None\n",
    "        )\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, g, feat, last_nodes):\n",
    "        if self.batch_norm is not None:\n",
    "            feat = self.batch_norm(feat)\n",
    "        feat = self.feat_drop(feat)\n",
    "        feat_u = self.fc_u(feat)\n",
    "        feat_v = self.fc_v(feat[last_nodes])\n",
    "        feat_v = dgl.broadcast_nodes(g, feat_v)\n",
    "        e = self.fc_e(torch.sigmoid(feat_u + feat_v)) \n",
    "        alpha = F.segment.segment_softmax(g.batch_num_nodes(), e) \n",
    "        feat_norm = feat * alpha\n",
    "        rst = F.segment.segment_reduce(g.batch_num_nodes(), feat_norm, 'sum')\n",
    "        if self.fc_out is not None:\n",
    "            rst = self.fc_out(rst)\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "        return rst\n",
    "    \n",
    "class NISER(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_items, embedding_dim, num_layers, feat_drop=0.0, norm=True, scale=12):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.register_buffer('indices', torch.arange(num_items, dtype=torch.long))\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norm = norm\n",
    "        self.scale = scale\n",
    "        input_dim = embedding_dim\n",
    "        for i in range(num_layers):\n",
    "            layer = SRGNNLayer(\n",
    "                input_dim,\n",
    "                embedding_dim,\n",
    "                batch_norm=None,\n",
    "                feat_drop=feat_drop\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        self.readout = AttnReadout(\n",
    "            input_dim,\n",
    "            embedding_dim,\n",
    "            embedding_dim,\n",
    "            batch_norm=None,\n",
    "            feat_drop=feat_drop,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.fc_sr = nn.Linear(input_dim + embedding_dim, embedding_dim, bias=False)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.embedding_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, mg, sg=None):\n",
    "        iid = mg.ndata['iid']\n",
    "        \n",
    "        feat = self.feat_drop(self.embedding(iid))\n",
    "        if self.norm:\n",
    "            feat = feat.div(torch.norm(feat, p=2, dim=-1, keepdim=True) + 1e-12)\n",
    "        out = feat\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out = layer(mg, out)\n",
    "            \n",
    "        last_nodes = mg.filter_nodes(lambda nodes: nodes.data['last'] == 1)\n",
    "        if self.norm:\n",
    "            feat = feat.div(torch.norm(out, p=2, dim=-1, keepdim=True))\n",
    "        sr_g = self.readout(mg, feat, last_nodes)\n",
    "        sr_l = feat[last_nodes]\n",
    "        sr = torch.cat([sr_l, sr_g], dim=1)\n",
    "        sr = self.fc_sr(sr)\n",
    "        if self.norm:\n",
    "            sr = sr.div(torch.norm(sr, p=2, dim=-1, keepdim=True) + 1e-12)\n",
    "        target = self.embedding(self.indices)\n",
    "        if self.norm:\n",
    "            target = target.div(torch.norm(target, p=2, dim=-1, keepdim=True) + 1e-12)\n",
    "        logits = sr @ target.t()\n",
    "        if self.scale:\n",
    "            logits = torch.log(nn.functional.softmax(self.scale * logits, dim=-1))\n",
    "        else:\n",
    "            logits = torch.log(nn.functional.softmax(logits, dim=-1))\n",
    "        return logits# , 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b928d32-e0fc-424e-b757-d93c8c5a520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NISER(num_items, args.embedding_dim, args.num_layers, feat_drop=args.feat_drop)\n",
    "num_items, args.embedding_dim, args.num_layers, args.feat_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c8549-c2ea-43ab-9821-b31a6ff96614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NISER(num_items, args.embedding_dim, args.num_layers, feat_drop=args.feat_drop)\n",
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f965f-8b0d-4f5f-b4e6-d23404a0b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49986035-b8fa-42aa-8fa1-0c8f8f0f7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc_sr.weight.shape, model.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba7e52-ebc5-4628-914b-4511fe8cd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([p.nelement() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d664c8a-da3b-4232-98cf-4c57c3d8d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(test_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783ece4-3a1b-4c1a-846c-6c7c88e98fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch, device):\n",
    "    inputs, labels = batch\n",
    "    # inputs, labels = batch\n",
    "    inputs_gpu  = [x.to(device) for x in inputs]\n",
    "    labels_gpu  = labels.to(device)\n",
    "   \n",
    "    return inputs_gpu, labels_gpu \n",
    "# inputs, labels = prepare_batch(batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46dfeb2-5a77-40f4-b985-53fb5238e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = batch\n",
    "mg=inputs[0]\n",
    "mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac462e-2cff-42a0-be82-eae2c06a4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.batch_num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2b75c-e4ef-4d3d-9650-ab44e0694152",
   "metadata": {},
   "outputs": [],
   "source": [
    "iid=mg.ndata['iid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbcaaf-8469-4233-903c-7d165f2af74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = model.feat_drop(model.embedding(iid))\n",
    "feat = feat.div(torch.norm(feat, p=2, dim=-1, keepdim=True) + 1e-12)\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71e3c3-1237-4c2f-9669-7306381c0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=model.layers[0]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb9dda-b403-4a57-bc6e-d545ecc1ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.ndata['ft']=layer.dropout(feat)\n",
    "mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8fa3a9-d96b-4da2-ba45-412b60747dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.edges_src['ft'].shape, mg.edata['w'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c905f-baac-4c84-b79f-0527993926f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402b540-1d65-4425-9edb-a5fd682301fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c6b9e-5d27-4dc3-bf13-9dff66c50cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "gru = nn.GRU(input_size = 8, hidden_size = 50, num_layers = 3, batch_first = True)\n",
    "inp = torch.randn(1024, 112, 8)\n",
    "out, hn = gru(inp)\n",
    "out.shape, hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4e5c8-9032-46d1-93c2-2de598b16cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(out[:,-1],hn[-1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9af46-06aa-4015-966e-e8bbda656f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SRGNN",
   "language": "python",
   "name": "srgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "00e4787372b5b2048923b123daa096330efcc5ff925c046928bc65d34a5b450a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
